{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d7e14a",
   "metadata": {},
   "source": [
    "For the last part of this assignment, we're going to migrate to a popular deep learning framework called PyTorch. We will train and analyze a convolutional neural network on iWildCam, a dataset that contains camera trap images (https://sites.google.com/view/fgvc6/competitions/iwildcam-2019). We've subsampled this dataset to use only 3 classes, rather than the full dataset. \n",
    "\n",
    "This section of the assignment is a lot more open ended than the previous parts, (and in fact, can serve as the starting point for your final project, if you're interested) \n",
    "\n",
    "\n",
    "**Why use a deep learning framework?**\n",
    "* Our code can now run on GPUs. You can use something like Google colab to use GPUs to train your code much faster. A framework like Pytorch interfaces directly with the GPU architecture without us having to write CUDA code directly (which is beyond the scope of this class).\n",
    "* In this class, we want you to be ready to use one of these framework for your project so you can experiment more efficiently than if you were writing every feature you want to use by hand. \n",
    "* We want you to stand on the shoulders of giants! PyTorch is an excellent framework that will make your lives a lot easier, and now that you understand the guts of convolutional networks, you should feel free to use such frameworks. \n",
    "* Finally, we want you to be exposed to the sort of deep learning code you might run into in academia or industry.\n",
    "\n",
    "**Note: We're going to be working at the highest level of abstraction within PyTorch. This should provide enough flexibility to be able to train a model for our purpose within this assignment, but you can do a lot more with PyTorch, if you're interested** You can go through this [tutorial](https://github.com/jcjohnson/pytorch-examples?tab=readme-ov-file) to understand more about the library itself.\n",
    "\n",
    "\n",
    "*Thanks to instructors from Stanfords' CS231n, including Prof. Fei-Fei Li, Prof. Ranjay Krishna, and Prof. Justin Johnson for ideas and some starter code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4308d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c21028",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "dtype = torch.float32 # We will be using float throughout.\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available(): \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa63efda",
   "metadata": {},
   "source": [
    "## Working with Data\n",
    "\n",
    "Pytorch offers dataloaders to handle datasets easily. Here is a Dataset class written for the iWildCam dataset. Note the two required functions: `__len__()` that gives the length of the dataset and `__getitem__()` which retrieves a specific image and it's annotations \n",
    "\n",
    "We're interested in two aspects of each iWildCam image: the kind of animal (stored in `labels`) that we're using as the target variable and the camera trap location (stored in `locations`) for our analysis. `__getitem__()` returns both of these annotations as well as the image itself. Also note the transform - we convert the image to a form that we want to work with. Typically, we want the image to be a **`tensor`**, which is similar to a numpy array. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5aacd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WildCamDataset(Dataset):\n",
    "    def __init__(self, img_paths, annotations, transform=T.ToTensor(), directory='WildCam_3classes/train'):\n",
    "        self.img_paths = img_paths\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "        self.dir = directory\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ID = '{}/{}'.format(self.dir, self.img_paths[index])\n",
    "        img = Image.open(ID).convert('RGB')\n",
    "        X = self.transform(img)             \n",
    "        y = self.annotations['labels'][self.img_paths[index]]\n",
    "        loc = self.annotations['locations'][self.img_paths[index]]\n",
    "        return X, y, loc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3733488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# We often want to preprocess the image. Here, we're resizing all images to 112x112 and then normalizing \n",
    "# them. You can add additional transforms / data augmentations here, check out torchvision transforms: \n",
    "# https://pytorch.org/vision/stable/transforms.html\n",
    "# Remember, if you do any kind of randomized data transformation during training, we need to find the (approximate)\n",
    "# expected value during that during inference. \n",
    "\n",
    "normalize = T.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "transform = T.Compose([\n",
    "            T.Resize((112,112)),\n",
    "            T.ToTensor(),\n",
    "            normalize\n",
    "])\n",
    "\n",
    "# We're also specifying the batch size and whether or not we want to shuffle the images \n",
    "# Typically, we shuffle the images when training, but we don't need to shuffle images when testing the model \n",
    "\n",
    "param_train = {\n",
    "    'batch_size': 256,       \n",
    "    'shuffle': True\n",
    "    }\n",
    "\n",
    "param_valtest = {\n",
    "    'batch_size': 256,\n",
    "    'shuffle': False\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec1bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip the subset of the dataset before running the following. \n",
    "# You may need to change the folder names / locations. \n",
    "\n",
    "annotations = json.load(open('WildCam_3classes/annotations.json'))\n",
    "\n",
    "train_images = sorted(os.listdir('WildCam_3classes/train'))\n",
    "train_dset = WildCamDataset(train_images, annotations, transform, directory='WildCam_3classes/train/')\n",
    "train_loader = DataLoader(train_dset, **param_train)\n",
    "\n",
    "val_images = sorted(os.listdir('WildCam_3classes/val'))\n",
    "val_dset = WildCamDataset(val_images, annotations, transform, directory=\"WildCam_3classes/val/\")\n",
    "val_loader = DataLoader(val_dset, **param_valtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc5ad8",
   "metadata": {},
   "source": [
    "## PyTorch Sequential API\n",
    "\n",
    "PyTorch provides a container Module called `nn.Sequential`, which allows us to construct simple, feedforward networks. It is not as flexible as other methods, but is sufficient for our case. \n",
    "\n",
    "\n",
    "### Sequential API: Two-Layer Network\n",
    "Let's see how to write a simple two-layer fully connected network example with `nn.Sequential`, and train it.\n",
    "You don't need to tune any hyperparameters here, but you shoud achieve above 60% accuracy after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts N 3 dimentional images to a one dimensional vector per image\n",
    "def flatten(x):\n",
    "    N = x.shape[0] \n",
    "    return x.view(N, -1)  \n",
    "\n",
    "# We need to wrap `flatten` function in a module in order to stack it in nn.Sequential\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "\n",
    "hidden_layer_size = 100\n",
    "learning_rate = 5e-4\n",
    "\n",
    "\n",
    "# This creates a model that has 2 linear layers with a hidden layer size of 100. \n",
    "# Notice that we first flatten our images\n",
    "model = nn.Sequential(\n",
    "    Flatten(),\n",
    "    nn.Linear(3 * 112 * 112, hidden_layer_size), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_layer_size, 3),\n",
    ")\n",
    "\n",
    "\n",
    "# optim has different optimizers you can try! This is set to SGD + momentum, but you can use Adam or \n",
    "# RMSprop as examples. See https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631a72cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loader_train, loader_val, epochs=1, print_every=1):\n",
    "    \"\"\"\n",
    "    Train a model using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - loader_train: A dataloader containing the train dataset\n",
    "    - loader_val: A dataloader containing the validation dataset\n",
    "    - epochs: (Optional) An integer giving the number of epochs to train for\n",
    "    - print_every: (Optional) An integer specifying how often to print the loss. \n",
    "    \n",
    "    Returns: Nothing, but prints model losses and accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y, loc) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = torch.nn.functional.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Epoch {}, iteration {}, loss = {}'.format(e, t, loss.item()))\n",
    "                \n",
    "        \n",
    "        print('Epoch {} done'.format(e))\n",
    "        check_accuracy(loader_val, model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b865d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_accuracy(loader, model):\n",
    "    \"\"\"\n",
    "    Finds the accuracy of a model\n",
    "    \n",
    "    Inputs:\n",
    "    - loader: A dataloader containing the validation / testing dataset\n",
    "    - model: A PyTorch Module giving the model to evaluate.\n",
    "    \n",
    "    Returns: Nothing, but prints the accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad(): # no need to store computation graph or local gradients\n",
    "        for x, y, loc in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395bf3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, train_loader, val_loader, print_every=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d69b598",
   "metadata": {},
   "source": [
    "### Sequential API: 3-layer ConvNet\n",
    "\n",
    "Here you should use `nn.Sequential` to define and train a three-layer ConvNet with this architecture. \n",
    "Hint: Look up https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html and https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d\n",
    "\n",
    "1. Convolutional layer (with bias) with 32 5x5 filters, with zero-padding of 2\n",
    "2. ReLU\n",
    "3. Convolutional layer (with bias) with 16 3x3 filters, with zero-padding of 1\n",
    "4. ReLU\n",
    "5. MaxPool (filter size 2, stride 2)\n",
    "5. Fully-connected layer (with bias) to compute scores for 3 classes\n",
    "\n",
    "You can use the default PyTorch weight initialization.\n",
    "\n",
    "You should optimize your model using stochastic gradient descent with momentum 0.9.\n",
    "\n",
    "Again, you don't need to tune any hyperparameters but you should see accuracy above 60% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "learning_rate = 7.5e-4\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "\n",
    "# TODO: Write a 3-layer ConvNet Sequential API.                            \n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "train(model, optimizer, train_loader, val_loader, print_every=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8e8e20",
   "metadata": {},
   "source": [
    "## Open ended challenge\n",
    "\n",
    "Now, it's your turn. Design and train a network that gets over **80% accuracy on the validation set** within **5 epochs**. You can play around with different architectures (e.g, increasing the depth , changing the number / size of filters), changing the optimizer (e.g., using Adam or RMSProp), adding data augmentation, etc. \n",
    "\n",
    "\n",
    "**Deliverable**: In your report, describe what you did. Make sure to include your model architecture, optimizer and list all hyperparameters. Additionally, plot the training loss across the iterations (you can modify the `train()` function to return the loss values)\n",
    "\n",
    "You can save your model as follows: \n",
    "\n",
    "```\n",
    "torch.save(model.state_dict(), PATH)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe91074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4544e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')  ## to save a model to model.pth\n",
    "\n",
    "\n",
    "# Suppose your model is the 2-layer fully connected network we defined initially. In that case, do the \n",
    "# following to load the parameter values into the model \n",
    "model = nn.Sequential(\n",
    "    Flatten(),\n",
    "    nn.Linear(3 * 112 * 112, hidden_layer_size), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_layer_size, 3),\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load('model.pth'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f227337e",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "We're now going to walk through an analysis of this model. First, create a dataloader that contains images from the `test` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a0ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create a test data loader, similar to what we did for the validation set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebfe450",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "\n",
    "Next, find the overall model accuracy on the test dataset, as well as the per-class accuracies. You may need to modify / rewrite `check_accuracy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the overall accuracy and the per-class accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427bcd04",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "Let's dig deeper into the analysis. Construct the confusion matrix for your model (you can use the sklearn implementation, if you prefer.) Are any of the classes harder to identify than the others? Do you have any hypotheses for these? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241e272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bfa460",
   "metadata": {},
   "source": [
    "#### Analyzing a single class by camera trap location\n",
    "\n",
    "Pick one of the three classes. Let's analyze how the model performance is different for different camera traps (this is the `location` information within the annotations). Let's find the per-class accuracy for this class for each of the different camera trap locations. Plot the accuracy as a function of the fraction of images from the training set that come from that location. What do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045c7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3eda265",
   "metadata": {},
   "source": [
    "Visualize images that are from camera trap locations with good performance versus those from locations with poor performance. What do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f406292e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb1b2866",
   "metadata": {},
   "source": [
    "#### Improvements\n",
    "\n",
    "Finally, describe how you might improve the model performance on rare camera trap locations. You don't have to actually implement your proposed improvement, but describe exactly how you could go about implementing it and what pitfalls you might anticipate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830fe24a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
